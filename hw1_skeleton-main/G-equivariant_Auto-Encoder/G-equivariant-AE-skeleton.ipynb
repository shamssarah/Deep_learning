{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning G-equivariant Representation\n",
    "\n",
    "In this question, we will learn a G-equivariant MLP Auto-Encoder.\n",
    "\n",
    "- Task: Simply reconstruct the input MNIST handwritten digit with your MLP.\n",
    "\n",
    "- Training data\n",
    "    - Consider the MNIST dataset of handwritten digits:\n",
    "<center><img src=\"example_image.png\" width=300px /></center>\n",
    "    - The above dataset contains mostly upright digits that we will give as input to our MLP training.\n",
    "\n",
    "- Now consider the following out-of-distribution test data\n",
    "    - Consider the transformation group $G_\\text{rot} \\equiv \\{T^{(\\theta)}\\}_{k\\in \\{0^\\circ,90^\\circ,180^\\circ,270^\\circ\\}}$, which rotates the image by $\\theta$ degrees, where $T^{(0)}$ is the identity transformation.\n",
    "    - The input to the transformation group is ${\\bf x}$, the **vectorized** image, and we aim to reconstruct the input image using our Auto-Encoder model\n",
    "    - The Kronecker product in pytorch can be invoked as ```torch.kron```\n",
    "\n",
    "- Task: Training a G-equivariant MLP auto-encoder over the standard (upright) MNIST data.\n",
    "    - That is, if your MLP receives a rotated MLP in the test data, it will try to output the same rotated image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard MLP trained on mostly upright images, will be confused when we give out-of-distribution (rotated) data. For instance output of a simple Auto-Encoder model that it does not work on rotated input:\n",
    "<center><img src=\"example-ae-output.png\" width=300px /></center>\n",
    "\n",
    "- We can observe that, for a standard MLP, the reconstructed output is a much worse for a rotated input than the original MNIST input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Task:\n",
    "**In what follows, you are required to fill in the code for constructing a G-equivairant Auto-Encoder model that is able to correct reconstruct the out-of-distribution (rotated) MNIST images.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first review some concepts from the class.\n",
    "### G-equivariant Representations\n",
    "\n",
    "- Consider an input in $\\mathbb{R}^m$.\n",
    "- And the output in $\\mathbb{R}^k$.\n",
    "- Let ${\\bf W} \\in \\mathbb{R}^{k \\times m}$ be the neuron parameters\n",
    "- Equivariance requires that transforming the input is the same as transforming the output: $$\n",
    "{\\bf x} \\in \\mathbb{R}^m, \\forall g \\in G , \\quad \\rho_2(g) {\\bf W} {\\bf x} =  {\\bf W} \\rho_1(g) {\\bf x},\n",
    "$$ where $\\rho_1:G \\to \\mathbb{R}^{\\sqrt{m} \\times \\sqrt{m}}$ and $\\rho_2:G \\to \\mathbb{R}^{\\sqrt{k} \\times \\sqrt{k}}$. (Thus we require $k$ and $m$ to be perfect square numbers.)\n",
    "- Since the above is true for all ${\\bf x}$, then  $$\\rho_2(g) {\\bf W} \\rho_1(g)^{-1} =  {\\bf W},$$\n",
    "or equivalently  $$\\forall g \\in G, \\qquad \\rho_2(g) \\otimes \\rho_1(g^{-1})^T \\text{vec}({\\bf W})  =  \\text{vec}({\\bf W}),$$ where vec flattens the matrix into a vector, and so the whole transformation is $\\rho_2(g) \\otimes \\rho_1(g^{-1})^T = \\rho_{12}(g)$ that is a representation of how $g$ acts on matrices mapping from $A_1 \\to A_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group-Equivariant ($G$-Equivariant) Neurons\n",
    "\n",
    "- Note that equation $$\\forall g \\in G, \\qquad \\rho_2(g) \\otimes \\rho_1(g^{-1})^T \\text{vec}({\\bf W})  =  \\text{vec}({\\bf W})$$ has a familiar form.\n",
    "    - We can rewrite the above equation as $$T {\\bf x} = {\\bf x},$$ where $T = \\rho_2(g) \\otimes \\rho_1(g^{-1})^T$ and ${\\bf x} = \\text{vec}({\\bf W}).$\n",
    "    \n",
    "- The above is the equation required for a $G$-invariant representation.\n",
    "    - We can again use the Reynolds operator to find $\\bar{T}$ such that $\\forall {\\bf x} \\in \\mathbb{R}^{mk}$ and $$\\forall T \\in \\{\\rho_2(g) \\otimes \\rho_1(g^{-1})^T : \\forall g \\in G\\}$$ such that $$\\bar{T}(T {\\bf x}) = \\bar{T} {\\bf x}.$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Get the eigenvectors of the symmetrization (Reynolds) operator\n",
    "\n",
    "- To obtain the Reynolds operator, we first need to construct the transformation matrix for $\\forall T \\in \\{\\rho_2(g) \\otimes \\rho_1(g^{-1})^T : \\forall g \\in G\\}$\n",
    "    - We will first calculate the transformation matrix for $\\rho_1(g)$ and $\\rho_2(g^{-1}), \\forall g\\in G$\n",
    "    - Then we will do the Kronecker product between $\\rho_1(g)$ and $\\rho_2(g^{-1}), \\forall g\\in G$\n",
    "    - As said in class, we will construct the transformation matrix through one-encoded vector\n",
    "    - Finally, we are able to get the matrix of the symmetrization (Reynolds) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 90-rotation function.\n",
    "def rotate_array(array, degree):\n",
    "    if (array.ndim == 2 and degree % 90 == 0):\n",
    "        return np.rot90(array, degree // 90)\n",
    "    else:\n",
    "        print(\"Can only rorate 2D array by 90n degrees.\")\n",
    "        raise RuntimeError\n",
    "        \n",
    "# Define the rotation over the one-hot vector with a specific degree and shape\n",
    "def transformed_onehot_degree(onehot_loc, transform_func, shape, degree):\n",
    "    # Get flattened onehot.\n",
    "    numel = np.prod(shape)\n",
    "    onehot = np.zeros(numel)\n",
    "    onehot[onehot_loc] = 1\n",
    "\n",
    "    # Transform onehot with specific shape view and flatten back.\n",
    "    transformed_vec = transform_func(onehot.reshape(*shape), degree)\n",
    "    return transformed_vec.reshape(numel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the function to obtain the equivariant subspace. \n",
    "**Note that there are missing parts in the code for you to fill in (marked with TODO).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of CPUs to use for subspace generation.\n",
    "NUM_TRANSFORM_CPUS = 4\n",
    "\n",
    "#Important variables\n",
    "#  - transform_mat is out Reynolds operator \\bar{T}\n",
    "#  - eigenvectors are the eigenvectors of \\bar{T}\n",
    "\n",
    "# Apply given transformation on onehot corresponding to each pixel of 2D image.\n",
    "# Use Python multiprocessing since it is work with numpy array.\n",
    "# If it is torch Tensor, it is recommended to use PyTorch wrapped version of\n",
    "# multiprocessing (torch.multiprocessing).\n",
    "# Here we need to both transforma map over the input shape and output shape\n",
    "def get_equivariant_subspace(transform_func, shape1, shape2):\n",
    "    # Use multiprocessing to get transformation matrix correspond to given\n",
    "    # transformation function by paralleling onehot dimensions.\n",
    "    buf = [] # store the transformation map for each degree rotation\n",
    "    for degree in (0, 90, 180, 270): # for each degree, we need to calculate $\\rho_2(g) \\otimes \\rho_1(g^{-1})^T$\n",
    "        n1 = int(np.sqrt(shape1))\n",
    "        assert n1*n1 == shape1  # the vector dimension must be a perfect square number\n",
    "        pool = mp.Pool(NUM_TRANSFORM_CPUS)\n",
    "        trans_mat1 = pool.map(\n",
    "            partial(\n",
    "                transformed_onehot_degree,\n",
    "                transform_func=transform_func, shape=(n1,n1), degree=360-degree,\n",
    "            ),\n",
    "            range(shape1),\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        trans_mat1 = np.stack(trans_mat1,axis=1) # calculate the transformation map for \\rho_1(g^{-1})\n",
    "        \n",
    "        n2 = int(np.sqrt(shape2))\n",
    "        assert n2*n2 == shape2  # the vector dimension must be a perfect square number\n",
    "        pool = mp.Pool(NUM_TRANSFORM_CPUS)\n",
    "        trans_mat2 = pool.map(\n",
    "            partial(\n",
    "                transformed_onehot_degree,\n",
    "                transform_func=transform_func, shape=(n2,n2), degree=degree,\n",
    "            ),\n",
    "            range(shape2),\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        trans_mat2 = np.stack(trans_mat2,axis=1) # calculate the transformation map for \\rho_2(g)\n",
    "        \n",
    "        # TODO: Fill in the space below \n",
    "        # To obtain the transformation map for $\\rho_2(g) \\otimes \\rho_1(g^{-1})^T$ for each degree rotation\n",
    "        trans_mat = <TODO: put your code here>\n",
    "        buf.append(trans_mat)\n",
    "    \n",
    "    transform_mat = sum(buf) / len(buf) \n",
    "    # To obtain the symmetrization (Reynolds) operator of the group {\\rho_2(g) \\otimes \\rho_1(g^{-1})^T : \\forall g \\in G\\}\n",
    "        \n",
    "\n",
    "    # Eigenvectors is used to describe the transformation subspace.\n",
    "    # Use right side of decomposition thus eigenvectors are located at rows.\n",
    "    # Transformation matrix is assumed to be real symmetric\n",
    "    # TODO: Fill in the space below to obtain the eigenvectors\n",
    "    _, eigenvalues, eigenvectors = <TODO: put your code here>\n",
    "\n",
    "    # Eigenvalues are sorted from high to low, thus for over rank eigenvectors,\n",
    "    # they are null eigenvectors assigned with 0 eigenvalues.\n",
    "    # We should focus on non-trival eigenvectors.\n",
    "    rank = np.linalg.matrix_rank(np.diag(eigenvalues),hermitian=True)\n",
    "    \n",
    "    eigenvectors = eigenvectors[:rank]\n",
    "    return transform_mat, eigenvectors.T, eigenvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors of the Reynolds Operator\n",
    "We show an example of the eigenvectors of the Reynolds Operator for input space $\\mathbb{R}^{14\\times 14}$ and output space $\\mathbb{R}^{6\\times 6}$. Students can rerun the cell below to verify if the eigenvetors you get match the correct eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSubspace(subspace, Wshape, ndim=-1, channels=False):\n",
    "    subspace = subspace.T\n",
    "\n",
    "    if ndim == -1:\n",
    "        ndim = subspace.shape[0]\n",
    "    subspace = subspace[:ndim]\n",
    "\n",
    "    ndim = subspace.shape[0]\n",
    "    maxCols = min(ndim, 4)\n",
    "\n",
    "    for j in range(ndim):\n",
    "        if j % maxCols == 0:\n",
    "            plt.show()\n",
    "            nCols = maxCols if ndim - j > maxCols else ndim - j\n",
    "            fig, axes = plt.subplots(1, nCols, figsize=(12 * nCols // 2, 9 // 2))\n",
    "            try:\n",
    "                axes[0]\n",
    "            except:\n",
    "                axes = [axes]\n",
    "\n",
    "        kernel = subspace[j]\n",
    "        kernel = kernel.reshape(*Wshape)\n",
    "\n",
    "        if len(kernel.shape) == 3:\n",
    "            kernel = kernel.transpose(1, 2, 0)\n",
    "            if channels:\n",
    "                kernel = np.concatenate([kernel[:, :, c] for c in range(kernel.shape[-1])], axis=1)\n",
    "                axes[j%maxCols].add_patch(patches.Rectangle((-0.45, -0.45), 2.95, 2.95, facecolor='none', linestyle='--', linewidth=2, edgecolor='tab:red'))\n",
    "                axes[j%maxCols].add_patch(patches.Rectangle((2.55, -0.45), 2.95, 2.95, facecolor='none', linestyle='--', linewidth=2, edgecolor='tab:green'))\n",
    "                axes[j%maxCols].add_patch(patches.Rectangle((5.55, -0.45), 2.95, 2.95, facecolor='none', linestyle='--', linewidth=2, edgecolor='tab:blue'))\n",
    "\n",
    "        axes[j%maxCols].imshow(kernel.round(decimals=6), cmap=\"Greys\")\n",
    "        axes[j%maxCols].set_xticklabels([0] + [0, 1, 2] * 3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "transform_mat, eigenvectors, eigenvalues = get_equivariant_subspace(rotate_array, 14*14, 6*6)\n",
    "\n",
    "print(f\"The Reynolds operator tranformation matrix has dimension {transform_mat.shape}\")\n",
    "print(f\"The subspace W is the subspace span by {eigenvectors.shape[1]} eigenvectors\")\n",
    "print(\"--- First 16 eigenvectors shown as images ---\")\n",
    "showSubspace(eigenvectors, (14*6, 14*6), ndim=16, channels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Auto-Encoder\n",
    "\n",
    "- **First, we will test the performance of a standard Auto-Encoder**\n",
    "- The Original Train data contains regular MNIST images (shrink the resolution to $14\\times 14$ to ease computation)\n",
    "- The In-distribution Original Test data contains regular MNIST images\n",
    "- The Out-of-distribution Rotated Test data contains rotated MNIST images (rotated at random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first construct a standard Auto-Encoder\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # We will later replace the linear layers with a G-equivariant layers.\n",
    "        # Rest of the code remains the same.\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Linear(14*14, 36), # We fix the latent dimension\n",
    "                nn.ELU(),\n",
    "                nn.Linear(36, 9),\n",
    "                nn.ELU()\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(9, 36),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(36, 14*14)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.decoder(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write our train and test functions here\n",
    "def train_ae(train_loader, model, loss_fn, optimizer, epoch, device):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "\n",
    "        # Reshaping the image to (-1, 14*14)\n",
    "        data = data.reshape(-1, 14*14)\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_batch = model(data) # reconstruct the dataset\n",
    "\n",
    "        loss = loss_fn(recon_batch, data)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.6f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "def test_ae(test_loader, model, loss_fn, device, rotation=False):\n",
    "    torch.manual_seed(42)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(test_loader):\n",
    "        \n",
    "        if rotation:\n",
    "            # we will randomly rotate the input to construct the rotated test dataset\n",
    "            k = torch.randint(1, 4, (1,)).item() # randomly select a k for the rotation 90*k\n",
    "            data = torch.rot90(data, k, [2, 3]) \n",
    "        # Reshaping the image to (-1, 14*14)\n",
    "        data = data.reshape(-1, 14*14)\n",
    "        data = data.to(device)\n",
    "\n",
    "\n",
    "        recon_batch = model(data) # reconstruct the dataset\n",
    "\n",
    "        loss = loss_fn(recon_batch, data)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    return test_loss/ len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then load the data and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# We will throw away the training labels during training\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "resize = transforms.Resize(size=(14, 14)) # We will shrink the size of the image for ease of computation\n",
    "transform = transforms.Compose([resize, transforms.ToTensor()])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='../data_equi', train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root='../data_equi', train=False, transform=transform, download=True)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "train_loader = DataLoader(dataset=mnist_train, batch_size=128, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(dataset=mnist_test, batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "ae = AutoEncoder().to(device)\n",
    "\n",
    "ae_optimizer = optim.Adam(ae.parameters(), lr=lr)\n",
    "ae_loss_fn = torch.nn.MSELoss() # We use MSE loss for Auto-Encoder reconstruction\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train_ae(train_loader, ae, ae_loss_fn, ae_optimizer, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test_ae(test_loader, ae, ae_loss_fn, device)\n",
    "test_rotat_loss = test_ae(test_loader, ae, ae_loss_fn, device, rotation=True)\n",
    "print(\"For a standard Auto-Encoder model\")\n",
    "print(\"In-distribution Test Loss\", \"{:.6f}\".format(test_loss))\n",
    "print(\"Out-of-distribution (Rotated)  Test Loss\", \"{:.6f}\".format(test_rotat_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the difference in loss for *in-distribution* and *out-of-distribution* test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-Equivariant Auto-Encoder\n",
    "\n",
    "- **First, we will test the performance of a G-equivariant Auto-Encoder**\n",
    "- The original training data contains (mostly) upright MNIST images (we scale the images to $14\\times 14$ to simplify the computation)\n",
    "- The **in-distribution test** data contains regular **test** MNIST images\n",
    "- The **out-of-distribution test** data contains rotated MNIST images (rotated at random)\n",
    "\n",
    "In the following, we define the GEquivariantLayer() and GEquivariantAutoEncoder() class. \n",
    "\n",
    "**Note that there are missing parts in the code for you to fill (marked with TODO).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEquivariantLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.output_dim = output_dim\n",
    "        # Load the left 1-eigenvectors of the Reynolds operator that we computed before.\n",
    "        _, eigenvectors, _ = get_equivariant_subspace(rotate_array, self.input_dim, self.output_dim)\n",
    "        eigentensor = torch.from_numpy(eigenvectors).float()\n",
    "        self.basis = eigentensor.T\n",
    "        \n",
    "        self.coeffs = nn.Parameter(torch.Tensor(self.basis.shape[0], 1))\n",
    "        # TODO: Fill the parameter dimension of the bias term\n",
    "        self.bias = nn.Parameter(torch.Tensor(<TODO: Put the CORRECT dimension here>, 1))\n",
    "        # WARNING: Careful with the bias so it does not break the equivariance.\n",
    "        \n",
    "        stdv = 1.0/np.sqrt(output_dim)\n",
    "        self.coeffs.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.zero_()\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input shape: torch.Size([minibatch, input_dim])\n",
    "        \n",
    "        if self.basis.device != X.device:\n",
    "            self.basis = self.basis.to(X.device)\n",
    "\n",
    "        # Construct weight w \\in \\mathcal{W} (the left 1-eigenspace)\n",
    "        #      using the current learnable coefficients.\n",
    "        # coeffs: (output_dim, n_basis, 1)\n",
    "        # basis  : (n_basis, input_dim)\n",
    "        # result after torch.mul : (output_dim, n_basis, input_dim)\n",
    "        # result after sum : (output_dim, input_dim)\n",
    "        # TODO: Fill in how we can construct the weights by the basis eigenvectors\n",
    "        weights = <TODO: Put your code here>\n",
    "        weights = weights.sum(dim=-2)\n",
    "        weights = weights.view(self.output_dim, self.input_dim)\n",
    "\n",
    "        # Output shape: torch.Size([minibatch, output_dim])\n",
    "        out = X @ weights.T + self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEquivariantAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Replace the every layer with a G-equivariant layer.\n",
    "        self.first_layer = GEquivariantLayer(14*14, 6*6)\n",
    "        self.second_layer = GEquivariantLayer(6*6, 3*3)\n",
    "        self.third_layer = GEquivariantLayer(3*3, 6*6)\n",
    "        self.fourth_layer = GEquivariantLayer(6*6, 14*14)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "                self.first_layer,\n",
    "                nn.ELU(),\n",
    "                self.second_layer,\n",
    "                nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "                self.third_layer,\n",
    "                nn.ELU(),\n",
    "                self.fourth_layer,\n",
    "        )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.decoder(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_eq_ae = GEquivariantAutoEncoder()\n",
    "ae_optimizer = optim.Adam(G_eq_ae.parameters(), lr=lr)\n",
    "ae_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train_ae(train_loader, G_eq_ae, ae_loss_fn, ae_optimizer, epoch, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = test_ae(test_loader, G_eq_ae, ae_loss_fn, device)\n",
    "test_rotat_loss = test_ae(test_loader, G_eq_ae, ae_loss_fn, device, rotation=True)\n",
    "print(\"For a G-equivariant Auto-Encoder\")\n",
    "print(\"In-distribution Test Loss\", \"{:.6f}\".format(test_loss))\n",
    "print(\"Out-of-distribution (Rotated) Test Loss\", \"{:.6f}\".format(test_rotat_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the OOD performance is the same as the in-distribution performance. An example can be visualized as such:\n",
    "<center><img src=\"example-G-equi-ae-output.png\" width=700px /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 2 rows and 3 columns of subplots\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "ae = ae.to('cpu')\n",
    "G_eq_ae = G_eq_ae.to('cpu')\n",
    "\n",
    "x_example = mnist_test[9][0] # you can use a different example here\n",
    "rot_x_example = torch.rot90(mnist_test[9][0], 3, [1, 2]) # rotate 270 degree\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "axs[0, 0].imshow(x_example.squeeze().numpy())\n",
    "axs[0, 0].set_title(\"Input\")\n",
    "axs[0, 1].imshow(ae(x_example.reshape(1,-1)).detach().numpy().reshape(14,14))\n",
    "axs[0, 1].set_title(\"AE Output\")\n",
    "axs[0, 2].imshow(G_eq_ae(x_example.reshape(1,-1)).detach().numpy().reshape(14,14))\n",
    "axs[0, 2].set_title(\"G-Equivariant AE Output\")\n",
    "axs[1, 0].set_title(\"Rotated Input\")\n",
    "axs[1, 0].imshow(rot_x_example.squeeze().numpy())\n",
    "axs[1, 1].set_title(\"AE Output\")\n",
    "axs[1, 1].imshow(ae(rot_x_example.reshape(1,-1)).detach().numpy().reshape(14,14))\n",
    "axs[1, 2].set_title(\"G-Equivariant AE Output\")\n",
    "axs[1, 2].imshow(G_eq_ae(rot_x_example.reshape(1,-1)).detach().numpy().reshape(14,14))\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('comparison.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: G-Invariant Attention Pooling\n",
    "\n",
    "Now we extend the G-equivariant autoencoder by adding **G-invariant attention pooling** to create a global embedding. This architecture combines:\n",
    "\n",
    "- **Local G-equivariant features** $\\mathbf{h}$: Features that transform predictably under group actions\n",
    "- **Global G-invariant embedding** $\\mathbf{g}$: A pooled representation that remains unchanged under group actions\n",
    "\n",
    "### Key Insight: Attention as Reynolds Operator\n",
    "\n",
    "The attention mechanism can be made G-invariant by computing attention over group orbits. Given G-equivariant features $\\mathbf{h} \\in \\mathbb{R}^k$, we compute:\n",
    "\n",
    "$$\\alpha_i = \\frac{\\exp(\\mathbf{q}^T \\rho(g_i) \\mathbf{h})}{\\sum_{j=1}^{|G|} \\exp(\\mathbf{q}^T \\rho(g_j) \\mathbf{h})} \\quad \\text{for each } g_i \\in G$$\n",
    "\n",
    "$$\\mathbf{g} = \\sum_{i=1}^{|G|} \\alpha_i \\cdot \\rho(g_i) \\mathbf{h}$$\n",
    "\n",
    "where $\\mathbf{q} \\in \\mathbb{R}^k$ is a **learnable query vector**.\n",
    "\n",
    "**Why is this G-invariant?** When we transform the input $\\mathbf{h} \\to \\rho(g')\\mathbf{h}$, the set of orbit samples $\\{\\rho(g_i)\\mathbf{h}\\}$ just gets permuted (due to group closure), so the weighted sum remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get Group Representation Matrices\n",
    "\n",
    "First, we need to obtain the group representation matrices $\\rho(g)$ for the feature space. These matrices describe how each group element transforms the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_representations(feature_dim):\n",
    "    \"\"\"\n",
    "    Get the group representation matrices rho(g) for the rotation group G.\n",
    "    \n",
    "    Args:\n",
    "        feature_dim: The dimension of the feature space (must be a perfect square, e.g., 36 = 6x6)\n",
    "    \n",
    "    Returns:\n",
    "        group_reps: List of torch tensors [rho(0°), rho(90°), rho(180°), rho(270°)]\n",
    "                    Each tensor has shape (feature_dim, feature_dim)\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(feature_dim))\n",
    "    assert n * n == feature_dim, \"feature_dim must be a perfect square\"\n",
    "    \n",
    "    group_reps = []\n",
    "    for degree in [0, 90, 180, 270]:\n",
    "        # Compute the representation matrix by applying rotation to one-hot vectors\n",
    "        pool = mp.Pool(NUM_TRANSFORM_CPUS)\n",
    "        rep_mat = pool.map(\n",
    "            partial(\n",
    "                transformed_onehot_degree,\n",
    "                transform_func=rotate_array, shape=(n, n), degree=degree,\n",
    "            ),\n",
    "            range(feature_dim),\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        rep_mat = np.stack(rep_mat, axis=1)  # Shape: (feature_dim, feature_dim)\n",
    "        group_reps.append(torch.from_numpy(rep_mat).float())\n",
    "    \n",
    "    return group_reps\n",
    "\n",
    "# Test: Get group representations for 6x6 = 36 dimensional features\n",
    "group_reps_36 = get_group_representations(36)\n",
    "print(f\"Number of group elements: {len(group_reps_36)}\")\n",
    "print(f\"Shape of each representation matrix: {group_reps_36[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement G-Invariant Attention Pooling\n",
    "\n",
    "**TODO: Fill in the missing code to implement G-invariant attention pooling.**\n",
    "\n",
    "The module should:\n",
    "1. Transform the input features by each group element: $\\rho(g_i) \\mathbf{h}$ for all $g_i \\in G$\n",
    "2. Compute attention scores using a learnable query: $s_i = \\mathbf{q}^T \\rho(g_i) \\mathbf{h}$\n",
    "3. Apply softmax to get attention weights: $\\alpha_i = \\text{softmax}(s_i)$\n",
    "4. Compute weighted sum: $\\mathbf{g} = \\sum_i \\alpha_i \\cdot \\rho(g_i) \\mathbf{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GInvariantAttentionPool(nn.Module):\n",
    "    \"\"\"\n",
    "    G-Invariant Attention Pooling Module.\n",
    "    \n",
    "    Given G-equivariant features h, computes a G-invariant global embedding g\n",
    "    by attending over the group orbit {rho(g_i) * h : g_i in G}.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim, group_reps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dim: Dimension of input features (must be perfect square)\n",
    "            group_reps: List of group representation matrices [rho(g) for g in G]\n",
    "                        Each matrix has shape (feature_dim, feature_dim)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_group_elements = len(group_reps)\n",
    "        \n",
    "        # Store group representations as a buffer (not a parameter)\n",
    "        # Stack into tensor of shape (|G|, feature_dim, feature_dim)\n",
    "        self.register_buffer('group_reps', torch.stack(group_reps, dim=0))\n",
    "        \n",
    "        # TODO: Initialize the learnable query vector\n",
    "        self.query = nn.Parameter(<TODO: Initialize query vector with appropriate shape>)\n",
    "        \n",
    "        # Initialize query with small random values\n",
    "        nn.init.normal_(self.query, mean=0, std=0.01)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Compute G-invariant global embedding via attention over group orbits.\n",
    "        \n",
    "        Args:\n",
    "            h: G-equivariant features of shape (batch_size, feature_dim)\n",
    "        \n",
    "        Returns:\n",
    "            g: G-invariant global embedding of shape (batch_size, feature_dim)\n",
    "            attention_weights: Attention weights of shape (batch_size, |G|) - for visualization\n",
    "        \"\"\"\n",
    "        batch_size = h.shape[0]\n",
    "        \n",
    "        # Step 1: Compute orbit samples rho(g_i) @ h for each g_i in G\n",
    "        # h has shape (batch, feature_dim)\n",
    "        # group_reps has shape (|G|, feature_dim, feature_dim)\n",
    "        # We want orbit_samples of shape (batch, |G|, feature_dim)\n",
    "        \n",
    "        # TODO: Compute orbit samples\n",
    "        orbit_samples = <TODO: Compute rho(g_i) @ h for all g_i in G>\n",
    "        \n",
    "        # Step 2: Compute attention scores using query\n",
    "        # TODO: Compute attention scores\n",
    "        scores = <TODO: Compute attention scores using self.query>\n",
    "        \n",
    "        # Step 3: Apply softmax to get attention weights\n",
    "        # TODO: Compute attention weights using softmax over the group dimension\n",
    "        attention_weights = <TODO: Apply softmax to scores>\n",
    "        \n",
    "        # Step 4: Compute weighted sum to get G-invariant embedding\n",
    "        # TODO: Compute the weighted sum\n",
    "        g = <TODO: Compute weighted sum of orbit samples>\n",
    "        \n",
    "        return g, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Constrained Combine Layer for End-to-End Equivariance\n",
    "\n",
    "When combining G-equivariant features $\\mathbf{h}$ with G-invariant features $\\mathbf{g}$, the weight matrix must be properly constrained:\n",
    "\n",
    "$$W = [W_h \\mid W_g] \\in \\mathbb{R}^{k \\times 2k}$$\n",
    "\n",
    "**Constraints:**\n",
    "1. $W_h$: Must satisfy $\\rho(g) W_h = W_h \\rho(g)$ (standard G-equivariance)\n",
    "2. $W_g$: Columns must be in the **invariant subspace** of $\\rho$, i.e., $\\rho(g) W_g = W_g$\n",
    "3. Bias $\\mathbf{b}$: Must also be in the invariant subspace\n",
    "\n",
    "**Key insight:** $W_g$ maps the invariant input $\\mathbf{g}$ into an equivariant output. But the only way to do this equivariantly is if the output is also invariant. So columns of $W_g$ must be fixed points under $\\rho(g)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invariant_subspace_basis(feature_dim):\n",
    "    \"\"\"\n",
    "    Compute orthonormal basis for the G-invariant subspace.\n",
    "    These are vectors v such that rho(g) @ v = v for all g in G.\n",
    "    \"\"\"\n",
    "    n = int(np.sqrt(feature_dim))\n",
    "    assert n * n == feature_dim\n",
    "    \n",
    "    # Get rotation representations\n",
    "    group_reps = []\n",
    "    for degree in [0, 90, 180, 270]:\n",
    "        pool = mp.Pool(NUM_TRANSFORM_CPUS)\n",
    "        rep_mat = pool.map(\n",
    "            partial(transformed_onehot_degree, transform_func=rotate_array, shape=(n, n), degree=degree),\n",
    "            range(feature_dim),\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        group_reps.append(np.stack(rep_mat, axis=1))\n",
    "    \n",
    "    # Reynolds operator: T_bar = (1/|G|) * sum_g rho(g)\n",
    "    T_bar = sum(group_reps) / len(group_reps)\n",
    "    \n",
    "    # SVD to get basis for image of T_bar (= invariant subspace)\n",
    "    U, S, Vh = np.linalg.svd(T_bar)\n",
    "    rank = np.sum(S > 1e-10)\n",
    "    \n",
    "    return torch.from_numpy(U[:, :rank]).float(), rank\n",
    "\n",
    "\n",
    "class GEquivariantCombineLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    G-Equivariant layer for combining [h || g] where h is equivariant and g is invariant.\n",
    "    \n",
    "    Properly constrains:\n",
    "    - W_h to the equivariant subspace (intertwiner)\n",
    "    - W_g columns to the invariant subspace\n",
    "    - bias to the invariant subspace\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, equivariant_dim, invariant_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert equivariant_dim == invariant_dim == output_dim, \"For simplicity, all dims must be equal\"\n",
    "        self.dim = output_dim\n",
    "        \n",
    "        # Get equivariant subspace for W_h (same as GEquivariantLayer)\n",
    "        _, equiv_eigenvectors, _ = get_equivariant_subspace(rotate_array, self.dim, self.dim)\n",
    "        self.equiv_basis = torch.from_numpy(equiv_eigenvectors).float().T  # Shape: (n_basis, dim*dim)\n",
    "        \n",
    "        # Get invariant subspace for W_g columns and bias\n",
    "        self.inv_basis, self.inv_rank = get_invariant_subspace_basis(self.dim)  # Shape: (dim, inv_rank)\n",
    "        \n",
    "        # Learnable coefficients\n",
    "        # W_h: parameterized by equivariant basis\n",
    "        self.coeffs_h = nn.Parameter(torch.Tensor(self.equiv_basis.shape[0], 1))\n",
    "        \n",
    "        # W_g: columns in invariant subspace, so W_g = inv_basis @ C\n",
    "        # C has shape (inv_rank, dim) -> W_g has shape (dim, dim)\n",
    "        self.coeffs_g = nn.Parameter(torch.Tensor(self.inv_rank, self.dim))\n",
    "        \n",
    "        # Bias: in invariant subspace, so bias = inv_basis @ b_coeffs\n",
    "        self.bias_coeffs = nn.Parameter(torch.Tensor(self.inv_rank))\n",
    "        \n",
    "        # Initialize\n",
    "        stdv = 1.0 / np.sqrt(output_dim)\n",
    "        self.coeffs_h.data.uniform_(-stdv, stdv)\n",
    "        self.coeffs_g.data.uniform_(-stdv, stdv)\n",
    "        self.bias_coeffs.data.zero_()\n",
    "    \n",
    "    def forward(self, h, g):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: G-equivariant features (batch, dim)\n",
    "            g: G-invariant features (batch, dim)\n",
    "        Returns:\n",
    "            y: G-equivariant output (batch, dim)\n",
    "        \"\"\"\n",
    "        # Move bases to correct device\n",
    "        if self.equiv_basis.device != h.device:\n",
    "            self.equiv_basis = self.equiv_basis.to(h.device)\n",
    "        if self.inv_basis.device != h.device:\n",
    "            self.inv_basis = self.inv_basis.to(h.device)\n",
    "        \n",
    "        # Construct W_h from equivariant basis\n",
    "        W_h = (self.coeffs_h * self.equiv_basis).sum(dim=0).view(self.dim, self.dim)\n",
    "        \n",
    "        # Construct W_g: columns in invariant subspace\n",
    "        # W_g = inv_basis @ coeffs_g, shape: (dim, inv_rank) @ (inv_rank, dim) = (dim, dim)\n",
    "        W_g = self.inv_basis @ self.coeffs_g\n",
    "        \n",
    "        # Construct bias in invariant subspace\n",
    "        bias = self.inv_basis @ self.bias_coeffs  # (dim,)\n",
    "        \n",
    "        # Compute output: y = W_h @ h + W_g @ g + bias\n",
    "        y = h @ W_h.T + g @ W_g.T + bias\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "# Test the layer\n",
    "print(\"Testing GEquivariantCombineLayer...\")\n",
    "combine_layer = GEquivariantCombineLayer(9, 9, 9)\n",
    "h_test = torch.randn(2, 9)\n",
    "g_test = torch.randn(2, 9)\n",
    "y_test = combine_layer(h_test, g_test)\n",
    "print(f\"Input h shape: {h_test.shape}, g shape: {g_test.shape}\")\n",
    "print(f\"Output shape: {y_test.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in combine_layer.parameters())}\")\n",
    "print(\"Layer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the G-Equivariant Autoencoder with Attention Pooling\n",
    "\n",
    "Now we combine:\n",
    "1. **G-equivariant encoder layers** → produces local features $\\mathbf{h}$\n",
    "2. **G-invariant attention pooling** → produces global embedding $\\mathbf{g}$\n",
    "3. **Combination MLP** → combines $[\\mathbf{h} \\| \\mathbf{g}]$ \n",
    "4. **G-equivariant decoder layers** → reconstructs the output\n",
    "\n",
    "**Note:** The concatenation $[\\mathbf{h} \\| \\mathbf{g}]$ has a specific transformation behavior:\n",
    "- Under group action $g$: $[\\rho(g)\\mathbf{h} \\| \\mathbf{g}]$ (local transforms, global stays fixed)\n",
    "- This is a block-diagonal transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEquivariantAttentionAE(nn.Module):\n",
    "    \"\"\"\n",
    "    G-Equivariant Autoencoder with G-Invariant Attention Pooling.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: G-equivariant layers producing local features h\n",
    "    - Attention Pool: Produces G-invariant global embedding g\n",
    "    - Combine Layer: Properly constrained to maintain equivariance\n",
    "    - Decoder: G-equivariant layers for reconstruction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # G-equivariant encoder (same as before)\n",
    "        self.encoder_layer1 = GEquivariantLayer(14*14, 6*6)  # 196 -> 36\n",
    "        self.encoder_layer2 = GEquivariantLayer(6*6, 3*3)     # 36 -> 9\n",
    "        \n",
    "        # Get group representations for the bottleneck dimension (3x3 = 9)\n",
    "        group_reps_9 = get_group_representations(9)\n",
    "        \n",
    "        # G-invariant attention pooling on bottleneck features\n",
    "        self.attention_pool = GInvariantAttentionPool(9, group_reps_9)\n",
    "        \n",
    "        # Properly constrained combine layer (replaces unconstrained MLP)\n",
    "        # This ensures end-to-end G-equivariance!\n",
    "        self.combine_layer = GEquivariantCombineLayer(9, 9, 9)\n",
    "        \n",
    "        # G-equivariant decoder (same as before)\n",
    "        self.decoder_layer1 = GEquivariantLayer(3*3, 6*6)    # 9 -> 36\n",
    "        self.decoder_layer2 = GEquivariantLayer(6*6, 14*14)  # 36 -> 196\n",
    "        \n",
    "        self.activation = nn.ELU()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to local G-equivariant features.\n",
    "        \n",
    "        Returns:\n",
    "            h: Local G-equivariant features (batch, 9)\n",
    "        \"\"\"\n",
    "        h = self.activation(self.encoder_layer1(x))\n",
    "        h = self.activation(self.encoder_layer2(h))\n",
    "        return h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass with attention pooling.\n",
    "        \n",
    "        Returns:\n",
    "            recon: Reconstructed output\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Encode to get local G-equivariant features\n",
    "        h = self.encode(x)  # (batch, 9)\n",
    "        \n",
    "        # Get G-invariant global embedding via attention pooling\n",
    "        g, attention_weights = self.attention_pool(h)  # g: (batch, 9)\n",
    "        \n",
    "        # Combine using properly constrained layer\n",
    "        z = self.activation(self.combine_layer(h, g))  # (batch, 9)\n",
    "        \n",
    "        # Decode\n",
    "        recon = self.activation(self.decoder_layer1(z))\n",
    "        recon = self.decoder_layer2(recon)\n",
    "        \n",
    "        return recon, attention_weights\n",
    "    \n",
    "    def forward_no_attention(self, x):\n",
    "        \"\"\"Forward pass returning only reconstruction (for compatibility with train/test functions).\"\"\"\n",
    "        recon, _ = self.forward(x)\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train and Evaluate the Attention-Augmented Model\n",
    "\n",
    "Train the G-equivariant autoencoder with attention pooling and compare it to the standard G-equivariant autoencoder from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified training function for attention model (handles tuple output)\n",
    "def train_ae_attention(train_loader, model, loss_fn, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.reshape(-1, 14*14)\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Use forward_no_attention for training (ignores attention weights)\n",
    "        recon_batch = model.forward_no_attention(data)\n",
    "        \n",
    "        loss = loss_fn(recon_batch, data)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.6f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "def test_ae_attention(test_loader, model, loss_fn, device, rotation=False):\n",
    "    torch.manual_seed(42)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(test_loader):\n",
    "        if rotation:\n",
    "            k = torch.randint(1, 4, (1,)).item()\n",
    "            data = torch.rot90(data, k, [2, 3]) \n",
    "        data = data.reshape(-1, 14*14)\n",
    "        data = data.to(device)\n",
    "\n",
    "        recon_batch = model.forward_no_attention(data)\n",
    "        loss = loss_fn(recon_batch, data)\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    return test_loss / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the G-Equivariant Autoencoder with Attention Pooling\n",
    "# NOTE: This cell will only work after you complete the TODOs above!\n",
    "\n",
    "G_eq_attn_ae = GEquivariantAttentionAE().to(device)\n",
    "attn_optimizer = optim.Adam(G_eq_attn_ae.parameters(), lr=lr)\n",
    "attn_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "print(\"Training G-Equivariant AE with Attention Pooling...\")\n",
    "for epoch in range(1, 20):\n",
    "    train_ae_attention(train_loader, G_eq_attn_ae, attn_loss_fn, attn_optimizer, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON OF ALL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Standard AE (from Part 1)\n",
    "ae_test = test_ae(test_loader, ae, ae_loss_fn, device)\n",
    "ae_rot_test = test_ae(test_loader, ae, ae_loss_fn, device, rotation=True)\n",
    "print(f\"\\nStandard Auto-Encoder:\")\n",
    "print(f\"  In-distribution Test Loss:  {ae_test:.6f}\")\n",
    "print(f\"  OOD (Rotated) Test Loss:    {ae_rot_test:.6f}\")\n",
    "\n",
    "# G-Equivariant AE (from Part 1)\n",
    "geq_test = test_ae(test_loader, G_eq_ae, ae_loss_fn, device)\n",
    "geq_rot_test = test_ae(test_loader, G_eq_ae, ae_loss_fn, device, rotation=True)\n",
    "print(f\"\\nG-Equivariant Auto-Encoder:\")\n",
    "print(f\"  In-distribution Test Loss:  {geq_test:.6f}\")\n",
    "print(f\"  OOD (Rotated) Test Loss:    {geq_rot_test:.6f}\")\n",
    "\n",
    "# G-Equivariant AE with Attention (Part 2)\n",
    "geq_attn_test = test_ae_attention(test_loader, G_eq_attn_ae, attn_loss_fn, device)\n",
    "geq_attn_rot_test = test_ae_attention(test_loader, G_eq_attn_ae, attn_loss_fn, device, rotation=True)\n",
    "print(f\"\\nG-Equivariant AE with Attention Pooling:\")\n",
    "print(f\"  In-distribution Test Loss:  {geq_attn_test:.6f}\")\n",
    "print(f\"  OOD (Rotated) Test Loss:    {geq_attn_rot_test:.6f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Attention Weights\n",
    "\n",
    "Visualize the learned attention weights $\\alpha_i$ for several test images. \n",
    "\n",
    "**Questions to consider:**\n",
    "- Are the weights uniform across group elements (0°, 90°, 180°, 270°)?\n",
    "- Do the weights vary depending on the input image?\n",
    "- What does this tell you about what the attention mechanism has learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(model, test_data, num_samples=8, device='cpu'):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for several test images.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GEquivariantAttentionAE model\n",
    "        test_data: Test dataset\n",
    "        num_samples: Number of samples to visualize\n",
    "        device: Device to use\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 3*num_samples))\n",
    "    \n",
    "    group_labels = ['0°', '90°', '180°', '270°']\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get a test image\n",
    "        img, label = test_data[i]\n",
    "        x = img.reshape(1, -1).to(device)\n",
    "        \n",
    "        # Get reconstruction and attention weights\n",
    "        with torch.no_grad():\n",
    "            recon, attn_weights = model(x)\n",
    "        \n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(img.squeeze().numpy(), cmap='gray')\n",
    "        axes[i, 0].set_title(f'Input (digit {label})')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot reconstruction\n",
    "        axes[i, 1].imshow(recon.cpu().numpy().reshape(14, 14), cmap='gray')\n",
    "        axes[i, 1].set_title('Reconstruction')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Plot attention weights as bar chart\n",
    "        weights = attn_weights.cpu().numpy().flatten()\n",
    "        bars = axes[i, 2].bar(group_labels, weights, color=['blue', 'orange', 'green', 'red'])\n",
    "        axes[i, 2].set_ylim(0, 1)\n",
    "        axes[i, 2].set_title('Attention Weights')\n",
    "        axes[i, 2].set_ylabel('Weight')\n",
    "        \n",
    "        # Add weight values on bars\n",
    "        for bar, w in zip(bars, weights):\n",
    "            axes[i, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                          f'{w:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attention_weights_visualization.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "# Visualize attention weights for test images\n",
    "print(\"Visualizing attention weights...\")\n",
    "visualize_attention_weights(G_eq_attn_ae, mnist_test, num_samples=8, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compare Reconstructions (In-distribution vs OOD)\n",
    "\n",
    "Compare reconstructions of all three models on both upright and rotated inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: All models on upright and rotated inputs\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "\n",
    "# Move models to CPU for visualization\n",
    "ae_cpu = ae.to('cpu')\n",
    "G_eq_ae_cpu = G_eq_ae.to('cpu')\n",
    "G_eq_attn_ae_cpu = G_eq_attn_ae.to('cpu')\n",
    "\n",
    "# Get example image\n",
    "example_idx = 9  # Change this to try different images\n",
    "x_example = mnist_test[example_idx][0]\n",
    "rot_x_example = torch.rot90(x_example, 3, [1, 2])  # 270 degree rotation\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Row 1: Upright input\n",
    "axs[0, 0].imshow(x_example.squeeze().numpy(), cmap='gray')\n",
    "axs[0, 0].set_title(\"Input (Upright)\")\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[0, 1].imshow(ae_cpu(x_example.reshape(1,-1)).detach().numpy().reshape(14,14), cmap='gray')\n",
    "axs[0, 1].set_title(\"Standard AE\")\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "axs[0, 2].imshow(G_eq_ae_cpu(x_example.reshape(1,-1)).detach().numpy().reshape(14,14), cmap='gray')\n",
    "axs[0, 2].set_title(\"G-Equivariant AE\")\n",
    "axs[0, 2].axis('off')\n",
    "\n",
    "recon_attn, _ = G_eq_attn_ae_cpu(x_example.reshape(1,-1))\n",
    "axs[0, 3].imshow(recon_attn.detach().numpy().reshape(14,14), cmap='gray')\n",
    "axs[0, 3].set_title(\"G-Equiv AE + Attention\")\n",
    "axs[0, 3].axis('off')\n",
    "\n",
    "# Row 2: Rotated input (OOD)\n",
    "axs[1, 0].imshow(rot_x_example.squeeze().numpy(), cmap='gray')\n",
    "axs[1, 0].set_title(\"Input (Rotated 270°)\")\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(ae_cpu(rot_x_example.reshape(1,-1)).detach().numpy().reshape(14,14), cmap='gray')\n",
    "axs[1, 1].set_title(\"Standard AE\")\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "axs[1, 2].imshow(G_eq_ae_cpu(rot_x_example.reshape(1,-1)).detach().numpy().reshape(14,14), cmap='gray')\n",
    "axs[1, 2].set_title(\"G-Equivariant AE\")\n",
    "axs[1, 2].axis('off')\n",
    "\n",
    "recon_rot_attn, _ = G_eq_attn_ae_cpu(rot_x_example.reshape(1,-1))\n",
    "axs[1, 3].imshow(recon_rot_attn.detach().numpy().reshape(14,14), cmap='gray')\n",
    "axs[1, 3].set_title(\"G-Equiv AE + Attention\")\n",
    "axs[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle(\"Comparison: Standard AE vs G-Equivariant AE vs G-Equiv AE with Attention\", fontsize=14)\n",
    "plt.savefig('final_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
